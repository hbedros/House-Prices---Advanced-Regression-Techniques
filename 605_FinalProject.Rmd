---
title: "DATA605_Final"
author: "Haig Bedros"
date: "2024-05-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### House Prices - Advanced Regression Techniques 

```{r}
library(readr)
library(e1071)
library(ggplot2)
library(dplyr)
```

```{r}
train_df <- read.csv('https://raw.githubusercontent.com/hbedros/House-Prices---Advanced-Regression-Techniques/main/data/train.csv')
test_df <- read.csv('https://raw.githubusercontent.com/hbedros/House-Prices---Advanced-Regression-Techniques/main/data/test.csv')
```

```{r}
head(train_df)
```

1. Pick one of the quanititative independent variables from the training data set (train.csv) , and define that variable as  X.   
2. Make sure this variable is skewed to the right!  
3. Pick the dependent variable and define it as  Y.

1. Quantitative Independent Variable (X)
Among your dataset fields, a good choice for a quantitative independent variable (predictor) is GrLivArea. This variable represents the above grade (ground) living area in square feet. This is a continuous variable and is likely to have a linear relationship with the property’s sale price, making it a suitable candidate for regression analysis. Let's define it as X:

But first, let's check if it's right skewed:
```{r}
# Calculate skewness
skewness_grlivarea <- skewness(train_df$GrLivArea)

skewness_grlivarea
```

```{r}
# the skewness of GrLivArea is approximately 1.13, it already exhibits right skewness.

# defining X variable
X <- train_df$GrLivArea
```

2. Dependent Variable (Y)
The dependent variable, which you want to predict, is SalePrice. This is the property's sale price in dollars and is the target variable in your regression analysis. As you aim to predict the sale price based on other variables, it is appropriate to define it as Y:
```{r}
Y <- train_df$SalePrice
```

```{r}
# Scatter plot to examine the relationship
plot(X, Y, main="Scatter Plot", xlab="GrLivArea", ylab="SalePrice")

# Fit initial model
model <- lm(SalePrice ~ GrLivArea, data=train_df)
summary(model)

# Check residuals
plot(model$residuals, main="Residuals of Model", ylab="Residuals", xlab="Index")
hist(model$residuals, main="Histogram of Residuals", xlab="Residuals", breaks=30)

```

**Probability.**  
Calculate as a minimum the below probabilities a through c.  Assume the small letter "x" is estimated as the 3d quartile of the X variable, and the small letter "y" is estimated as the 2d quartile of the Y variable.  Interpret the meaning of all probabilities.  In addition, make a table of counts as shown below.

a.  P(X>x | Y>y)		b.  P(X>x, Y>y)		c.  P(X<x | Y>y)		

```{r}
# X <- train_df$GrLivArea
# Y <- train_df$SalePrice

# Load your data
# train_df <- read.csv("path/to/your/data.csv")

# Step 1: Calculate quartiles
x_3rd_quartile <- quantile(X, 0.75)
y_2nd_quartile <- quantile(Y, 0.5)

# Step 2: Create flags
train_df$X_gt_x <- ifelse(X > x_3rd_quartile, 1, 0)
train_df$Y_gt_y <- ifelse(Y > y_2nd_quartile, 1, 0)

# Step 3: Build contingency table
table <- table(train_df$X_gt_x, train_df$Y_gt_y)
print(table)

# Step 4: Calculate probabilities
# P(X > x | Y > y)
p_X_gt_x_given_Y_gt_y <- table[2, 2] / sum(table[, 2])

# P(X > x, Y > y)
p_X_gt_x_and_Y_gt_y <- table[2, 2] / sum(table)

# P(X < x | Y > y)
p_X_lt_x_given_Y_gt_y <- table[1, 2] / sum(table[, 2])

# Print probabilities
# print(paste("x_3rd_quartile:", x_3rd_quartile))
# print(paste("y_2nd_quartile:", y_2nd_quartile))
print(paste("P(X > x | Y > y):", p_X_gt_x_given_Y_gt_y))
print(paste("P(X > x, Y > y):", p_X_gt_x_and_Y_gt_y))
print(paste("P(X < x | Y > y):", p_X_lt_x_given_Y_gt_y))

```

```{r}
# Assuming train_df is your correct data frame
library(dplyr)

# Calculate quartiles
x_3rd_quartile <- quantile(train_df$GrLivArea, 0.75, na.rm = TRUE)
y_2nd_quartile <- quantile(train_df$SalePrice, 0.5, na.rm = TRUE)

# Add classifications based on quartiles
train_df <- train_df %>%
  mutate(
    X_class = if_else(GrLivArea <= x_3rd_quartile, "<=3d quartile", ">3d quartile"),
    Y_class = if_else(SalePrice <= y_2nd_quartile, "<=2d quartile", ">2d quartile")
  )

# Build contingency table
contingency_table <- table(train_df$X_class, train_df$Y_class)

# Adding row and column totals
contingency_table <- addmargins(contingency_table)

# Display the contingency table
print(contingency_table)

```

Does splitting the training data in this fashion make them independent?  
- Let A be the new variable counting those observations above the 3d quartile for X,  
- and let B be the new variable counting those observations above the 2d quartile for Y.  

Does P(A|B)=P(A)P(B)? Check mathematically, and then evaluate by running a Chi Square test for association.

**Mathematical Check of Independence**
```{r}
A <- 365
B <- 728
total_ob <- 1460

P_A <- A/total_ob
P_B <- B/total_ob

# to calculate P(A|B) we need P(A∩B) that both A and B are true and divide it by P(B)
P_A_B <- 315/total_ob

# now P(A|B)
P_A_given_B <- P_A_B/P_B

P_A_time_P_B <- P_A * P_B

# Print the results
cat("P(A):", P_A, "\n")
cat("P(B):", P_B, "\n")
cat("P(A|B):", P_A_given_B, "\n")
cat("P(A) * P(B):", P_A_time_P_B, "\n")
```
P(A|B): 0.4326923 =! P(A) * P(B): 0.1246575 this suggests that events A and B are not independent.

**Chi-squared test of independence**

```{r}
chi_square_test <- chisq.test(contingency_table)

chi_square_test
```
**p-value < 0.05:** The p-value is significantly less than 0.05, which mean we reject the null hypothesis of A and B being dependent.  

**In conclusions:** Splitting the data by quartiles in this manner does not make the variables GrLivArea and SalePrice independent. Both the mathematical check and the Chi-Square test confirm that there is a significant association between these variables.

**Descriptive and Inferential Statistics.**  
**Provide univariate descriptive statistics and appropriate plots for the training data set.**    

```{r}
summary(train_df)

# defining some continuous variables for visualization
cont_vars <- c("GrLivArea", "SalePrice", "LotArea", "TotalBsmtSF", "X1stFlrSF", "GarageArea")

# Histograms for continuous variables
for (var in cont_vars) {
  hist(train_df[[var]], main = paste("Histogram of", var), xlab = var, col = "blue", border = "black")
}

# defining some categorical variables for visualization
cat_vars <- c("MSZoning", "Street", "LotShape", "Neighborhood", "BldgType")

# Bar plots for categorical variables
for (var in cat_vars) {
  barplot(table(train_df[[var]]), main = paste("Bar Plot of", var), xlab = var, col = "purple", border = "black")
}
```
**Provide a scatterplot of X and Y.**    

```{r}
# scatter plot for X and Y from the first part
plot(X, Y, main="Scatter Plot", xlab="GrLivArea", ylab="SalePrice")

```
**Provide a 95% CI for the difference in the mean of the variables.**    
```{r}
# Assuming train_df is your training dataset
# GrLivArea, SalePrice

# Compute means and standard deviations
mean_GrLivArea <- mean(train_df$GrLivArea, na.rm = TRUE)
# mean_GarageArea <- mean(train_df$GarageArea, na.rm = TRUE)
mean_SalePrice <- mean(train_df$SalePrice, na.rm = TRUE)

sd_GrLivArea <- sd(train_df$GrLivArea, na.rm = TRUE)
# sd_GarageArea <- sd(train_df$GarageArea, na.rm = TRUE)
sd_SalePrice <- sd(train_df$SalePrice, na.rm = TRUE)

# Number of observations
n_GrLivArea <- sum(!is.na(train_df$GrLivArea))
n_SalePrice<- sum(!is.na(train_df$SalePrice))

# Compute the standard error of the difference
se_diff <- sqrt((sd_GrLivArea^2 / n_GrLivArea) + (sd_SalePrice^2 / n_SalePrice))

# Degrees of freedom (approximation)
df <- min(n_GrLivArea - 1, n_SalePrice - 1)

# Critical value for 95% CI from t-distribution
t_crit <- qt(0.975, df)

# Compute the margin of error
margin_of_error <- t_crit * se_diff

# Compute the difference in means
diff_means <- mean_GrLivArea - mean_SalePrice

# Compute the 95% confidence interval
lower_bound <- diff_means - margin_of_error
upper_bound <- diff_means + margin_of_error

# Print the results
cat("Mean of GrLivArea:", mean_GrLivArea, "\n")
cat("Mean of SalePrice:", mean_SalePrice, "\n")
cat("Difference in means:", diff_means, "\n")
cat("95% Confidence Interval for the difference in means: [", lower_bound, ",", upper_bound, "]\n")

```

**Derive a correlation matrix for two of the quantitative variables you selected.**    
```{r}
# Select the two quantitative variables
variables <- train_df %>% select(GrLivArea, SalePrice)

# Compute the correlation matrix
correlation_matrix <- cor(variables, use = "complete.obs")

# Print the correlation matrix
print(correlation_matrix)

```

**Test the hypothesis that the correlation between these variables is 0 and provide a 99% confidence interval.**    
```{r}
# H0 The correlation between GrLivArea and SalePrice is 0
# Ha The correlation between GrLivArea and SalePrice is NOT 0

# Perform the correlation test
cor_test <- cor.test(train_df$GrLivArea, train_df$SalePrice, conf.level = 0.99)

# Extract the correlation coefficient, p-value, and confidence interval
correlation_coefficient <- cor_test$estimate
p_value <- cor_test$p.value
conf_interval <- cor_test$conf.int

# Print the results
cat("Correlation Coefficient:", correlation_coefficient, "\n")
cat("P-value:", p_value, "\n")
cat("99% Confidence Interval:", conf_interval, "\n")


```

**Discuss the meaning of your analysis.**    

**Correlation Coefficient: 0.7086245**  

- There is a strong positive linear relationship between GrLivArea and SalePrice. This means that as the above-grade living area increases, the sale price tends to increase as well.  

**P-value: 4.518034e-223**  

- The p-value is extremely small (essentially zero), much smaller than the typical significance level (e.g., 0.01 for a 99% confidence interval).  
- This indicates that the correlation is highly statistically significant. We reject the null hypothesis that the correlation between GrLivArea and SalePrice is zero.  

**99% Confidence Interval: [0.6733974, 0.7406408]**  

- We are 99% confident that the true correlation coefficient lies between 0.6733974 and 0.7406408.  
- This range indicates a strong positive correlation, reaffirming the strength of the relationship between the two variables.  

**Conclusion**
Based on the results from the correlation test and the confidence interval:  

- There is a strong, statistically significant positive correlation between the living area above grade (GrLivArea) and the sale price (SalePrice).  
- The 99% confidence interval for the correlation coefficient is [0.6733974, 0.7406408], indicating a strong and reliable positive relationship between these variables.  

**Linear Algebra and Correlation.**  
Invert your correlation matrix. (This is known as the precision matrix and contains variance inflation factors on the diagonal.)  
```{r}
# First, we calculate the correlation matrix for GrLivArea and SalePrice. Then, we invert this matrix and extract the diagonal elements (VIFs).

# Select the two quantitative variables
variables <- train_df %>% select(GrLivArea, SalePrice)

# Compute the correlation matrix
correlation_matrix <- cor(variables, use = "complete.obs")

# Invert the correlation matrix to obtain the precision matrix
precision_matrix <- solve(correlation_matrix)

# Extract the diagonal elements (VIFs)
vifs <- diag(precision_matrix)

# Print the results
cat("Correlation Matrix:\n")
print(correlation_matrix)
cat("\nPrecision Matrix (Inverse of Correlation Matrix):\n")
print(precision_matrix)
cat("\nVariance Inflation Factors (VIFs):\n")
print(vifs)

```
**Interpretation**  
- The correlation matrix shows a strong positive correlation of 0.7086245 between GrLivArea and SalePrice. This indicates that as the above-grade living area increases, the sale price tends to increase as well.

- The precision matrix (inverse of the correlation matrix) has diagonal elements greater than 1 (2.008632), which is typical when there's some collinearity. The off-diagonal elements are negative, reflecting the inverse relationship within the context of the precision matrix. 

- VIFs: Both GrLivArea and SalePrice have a VIF of approximately 2.008632.
A VIF value close to 1 suggests low collinearity, while higher values indicate more severe collinearity. In this case, a VIF of 2.008632 indicates moderate collinearity. This means that there is some multicollinearity between GrLivArea and SalePrice, but it is not severe. 


Multiply the correlation matrix by the precision matrix, and then multiply the precision matrix by the correlation matrix.  
```{r}
# Multiplying the correlation matrix by the precision matrix (its inverse) and then the precision matrix by the correlation matrix should both yield the identity matrix, as the precision matrix is the inverse of the correlation matrix.

# Invert the correlation matrix to obtain the precision matrix
precision_matrix <- solve(correlation_matrix)

# Multiply correlation matrix by precision matrix
identity_matrix_1 <- correlation_matrix %*% precision_matrix

# Multiply precision matrix by correlation matrix
identity_matrix_2 <- precision_matrix %*% correlation_matrix

# Print the results
cat("Correlation Matrix:\n")
print(correlation_matrix)
cat("\nPrecision Matrix (Inverse of Correlation Matrix):\n")
print(precision_matrix)
cat("\nCorrelation Matrix * Precision Matrix:\n")
print(identity_matrix_1)
cat("\nPrecision Matrix * Correlation Matrix:\n")
print(identity_matrix_2)

```
The strong positive correlation (0.7086245) between GrLivArea and SalePrice indicates that larger living areas are associated with higher sale prices. The variance inflation factors (VIFs) around 2.008632 suggest moderate collinearity. Matrix multiplications confirm the precision matrix as the correct inverse of the correlation matrix.


Conduct principle components analysis (research this!)  and interpret.  Discuss.  
```{r}
# PCA

# We will perform PCA on GrLivArea and SalePrice from train_df.
# Load necessary library
library(stats)
library(ggbiplot)
library(ggplot2)

# Perform PCA
pca_result <- prcomp(variables, scale. = TRUE)

# Summary of PCA
summary(pca_result)

# Print PCA loadings
pca_result$rotation

# Print PCA scores
pca_result$x

# Plot PCA
library(ggplot2)
biplot(pca_result)

```
Interpretation:

- The first principal component (PC1) captures 85.43% of the total variance in the data, which means it represents the most significant trend or pattern in the dataset.   

- The second principal component (PC2) captures the remaining 14.57% of the variance, indicating that it captures a secondary trend or pattern.  

- Both GrLivArea and SalePrice contribute equally to PC1, and the negative signs indicate that as PC1 increases, both GrLivArea and SalePrice decrease. This suggests that PC1 captures the overall scale of the properties, where larger values (in absolute terms) of PC1 correspond to properties with larger living areas and higher sale prices.  
- For PC2, GrLivArea and SalePrice have opposite signs. This means that PC2 captures the contrast between GrLivArea and SalePrice. Higher PC2 scores indicate properties with relatively larger living areas but lower sale prices, and vice versa.  

- The PCA biplot shows that GrLivArea and SalePrice both contribute equally to the primary trend (PC1), representing a combined measure of property size and value, while PC2 highlights the inverse relationship between them, indicating differences between properties with larger living areas but relatively lower prices and vice versa. The cluster of points around the center suggests that most properties have similar characteristics in terms of their living area and sale price, with a few properties showing variations as indicated by their spread along the principal components.    
