---
title: "DATA605 Final Project"
author: "Haig Bedros"
date: "2024-05-13"
output:
  html_document:
    includes:
      in_header: header.html
    css: ./lab.css
    highlight: pygments
    theme: cerulean
    toc: true
    toc_float: true
  pdf_document: default
editor_options:
  chunk_output_type: console
---

```{r global-options, include=FALSE}
knitr::opts_chunk$set(eval = TRUE, message = FALSE, warning = FALSE)
library(readr)
library(e1071)
library(ggplot2)
library(dplyr)
library(caret)
library(stats)
library(ggbiplot)
library(MASS)
```

# House Prices - Advanced Regression Techniques

## Loading the training and test datasets
```{r}
train_df <- read.csv('https://raw.githubusercontent.com/hbedros/House-Prices---Advanced-Regression-Techniques/main/data/train.csv')
test_df <- read.csv('https://raw.githubusercontent.com/hbedros/House-Prices---Advanced-Regression-Techniques/main/data/test.csv')
```

## A quick glance on the training dataset
```{r}
head(train_df)
```

## 1. Selecting Analysis Variables

### 1.1 Quantitative Independent Variable (X) and its right skeweness

A good choice for a quantitative independent variable (predictor) is GrLivArea. This variable represents the ground living area in square feet. This is a continuous variable and is likely to have a linear relationship with the property’s sale price, making it a suitable candidate for our regression analysis.

But first, let's check if it's right skewed:  
```{r}
# Calculating skewness
skewness_grlivarea <- skewness(train_df$GrLivArea)

skewness_grlivarea
```
The skewness of `GrLivArea` is approximately 1.13, it already exhibits right skewness.  

```{r}
# Defining X variable
X <- train_df$GrLivArea
```

### 1.2 Selecting the dependent variable (Y)  

The dependent variable, which we want to predict, is `SalePrice`. This is the property's sale price in dollars and is the target variable in our regression analysis.  

We aim to predict the sale price based on other variables and we will define it as Y:  
```{r}
Y <- train_df$SalePrice
```

### 1.3 plotting the relationship between X and Y variables
```{r}
# Scatter plot to examine the relationship
plot(X, Y, main="Scatter Plot", xlab="GrLivArea", ylab="SalePrice")

# Fit initial model
model <- lm(SalePrice ~ GrLivArea, data=train_df)
summary(model)

# Check residuals
plot(model$residuals, main="Residuals of Model", ylab="Residuals", xlab="Index")
hist(model$residuals, main="Histogram of Residuals", xlab="Residuals", breaks=30)

```

## 2. Probability  

### 2.1 Probability Calculating Excersices and Creating a Table of Counts  

**Calculate as a minimum the below probabilities a through c.  Assume the small letter "x" is estimated as the 3d quartile of the X variable, and the small letter "y" is estimated as the 2d quartile of the Y variable.  Interpret the meaning of all probabilities.  In addition, make a table of counts as shown below.**  

**a.  P(X>x | Y>y)		b.  P(X>x, Y>y)		c.  P(X<x | Y>y)**		
```{r}
# X <- train_df$GrLivArea
# Y <- train_df$SalePrice

# Quartiles
x_3rd_quartile <- quantile(X, 0.75)
y_2nd_quartile <- quantile(Y, 0.5)

train_df$X_gt_x <- ifelse(X > x_3rd_quartile, 1, 0)
train_df$Y_gt_y <- ifelse(Y > y_2nd_quartile, 1, 0)

# Table of counts
table <- table(train_df$X_gt_x, train_df$Y_gt_y)
print(table)

# Probabilities:
# a. P(X > x | Y > y)
p_X_gt_x_given_Y_gt_y <- table[2, 2] / sum(table[, 2])

# b. P(X > x, Y > y)
p_X_gt_x_and_Y_gt_y <- table[2, 2] / sum(table)

# c. P(X < x | Y > y)
p_X_lt_x_given_Y_gt_y <- table[1, 2] / sum(table[, 2])

print(paste("a. P(X > x | Y > y):", p_X_gt_x_given_Y_gt_y))
print(paste("b. P(X > x, Y > y):", p_X_gt_x_and_Y_gt_y))
print(paste("c. P(X < x | Y > y):", p_X_lt_x_given_Y_gt_y))
```
**Table of Counts:**  
```{r}
# Calculate quartiles
x_3rd_quartile <- quantile(train_df$GrLivArea, 0.75, na.rm = TRUE)
y_2nd_quartile <- quantile(train_df$SalePrice, 0.5, na.rm = TRUE)

# Classifications based on quartiles
train_df <- train_df %>%
  mutate(
    X_class = if_else(GrLivArea <= x_3rd_quartile, "<=3d quartile", ">3d quartile"),
    Y_class = if_else(SalePrice <= y_2nd_quartile, "<=2d quartile", ">2d quartile")
  )

# Table of Counts
counts_table <- table(train_df$X_class, train_df$Y_class)
counts_table <- addmargins(counts_table)
print(counts_table)

```
### 2.2 Independence Analysis of Quartile-Based Splitting

**Does splitting the training data in this fashion make them independent?**  
- **Let A be the new variable counting those observations above the 3d quartile for X,**  
- **and let B be the new variable counting those observations above the 2d quartile for Y.**  

**Does P(A|B)=P(A)P(B)? Check mathematically, and then evaluate by running a Chi Square test for association.**  

#### 2.2.1 *Mathematical Check of Independence:*
```{r}
A <- 365
B <- 728
total_ob <- 1460

P_A <- A/total_ob
P_B <- B/total_ob

# to calculate P(A|B) we need P(A∩B) that both A and B are true and divide it by P(B)
P_A_B <- 315/total_ob

# now P(A|B)
P_A_given_B <- P_A_B/P_B

P_A_time_P_B <- P_A * P_B

cat("P(A):", P_A, "\n")
cat("P(B):", P_B, "\n")
cat("P(A|B):", P_A_given_B, "\n")
cat("P(A) * P(B):", P_A_time_P_B, "\n")
``` 
- P(A|B): 0.4326923 ≠ P(A) * P(B): 0.1246575 this suggests that events A and B are not independent.

#### 2.2.2 *Chi-squared test of independence:*
```{r}
chi_square_test <- chisq.test(counts_table)

chi_square_test
```
- p-value < 0.05: The p-value is significantly less than 0.05, which mean we reject the null hypothesis of A and B being dependent. 

#### 2.2.3 *Conclusions:*  
Splitting the data by quartiles does not make the variables GrLivArea and SalePrice independent. Both the mathematical check and the Chi-Square test confirm that there is a significant association between these variables.

## 3. Descriptive and Inferential Statistics

### 3.1 Univariate descriptive statistics and appropriate plots for the training data set    
```{r}
# defining some continuous variables for visualization
cont_vars <- c("GrLivArea", "SalePrice", "LotArea", "TotalBsmtSF", "X1stFlrSF", "GarageArea")

# Histograms for continuous variables
for (var in cont_vars) {
  hist(train_df[[var]], main = paste("Histogram of", var), xlab = var, col = "blue", border = "black")
}

# defining some categorical variables for visualization
cat_vars <- c("MSZoning", "Street", "LotShape", "Neighborhood", "BldgType")

# Bar plots for categorical variables
for (var in cat_vars) {
  barplot(table(train_df[[var]]), main = paste("Bar Plot of", var), xlab = var, col = "purple", border = "black")
}
```

#### Scatterplot of X and Y  

```{r}
# scatter plot for X and Y from the first part
plot(X, Y, main="Scatter Plot", xlab="GrLivArea", ylab="SalePrice")

```

#### 95% CI for the difference in the mean of the variables   

```{r}
# Means and Standard deviations
mean_GrLivArea <- mean(train_df$GrLivArea, na.rm = TRUE)
mean_SalePrice <- mean(train_df$SalePrice, na.rm = TRUE)

sd_GrLivArea <- sd(train_df$GrLivArea, na.rm = TRUE)
sd_SalePrice <- sd(train_df$SalePrice, na.rm = TRUE)

# Number of observations
n_GrLivArea <- sum(!is.na(train_df$GrLivArea))
n_SalePrice<- sum(!is.na(train_df$SalePrice))

# Standard error of the difference
se_diff <- sqrt((sd_GrLivArea^2 / n_GrLivArea) + (sd_SalePrice^2 / n_SalePrice))

# Degrees of freedom
df <- min(n_GrLivArea - 1, n_SalePrice - 1)

# 95% CI
t_crit <- qt(0.975, df)

# Margin of error
margin_of_error <- t_crit * se_diff

# Difference in means
diff_means <- mean_GrLivArea - mean_SalePrice

# 95% confidence interval
lower_bound <- diff_means - margin_of_error
upper_bound <- diff_means + margin_of_error

cat("Mean of GrLivArea:", mean_GrLivArea, "\n")
cat("Mean of SalePrice:", mean_SalePrice, "\n")
cat("Difference in means:", diff_means, "\n")
cat("95% Confidence Interval for the difference in means: [", lower_bound, ",", upper_bound, "]\n")
```

#### Derive a correlation matrix for two of the quantitative variables you selected.    

```{r}
# Selecting our two quantitative variables
variables <- train_df[, c("GrLivArea", "SalePrice")]

# Correlation matrix
correlation_matrix <- cor(variables, use = "complete.obs")

print(correlation_matrix)
```

### 3.2 Hypothesis testing  

**Test the hypothesis that the correlation between these variables is 0 and provide a 99% confidence interval.**    
```{r}
# H0 The correlation between GrLivArea and SalePrice is 0
# Ha The correlation between GrLivArea and SalePrice is NOT 0

# Perform the correlation test
cor_test <- cor.test(train_df$GrLivArea, train_df$SalePrice, conf.level = 0.99)

# Extract the correlation coefficient, p-value, and confidence interval
correlation_coefficient <- cor_test$estimate
p_value <- cor_test$p.value
conf_interval <- cor_test$conf.int

# Print the results
cat("Correlation Coefficient:", correlation_coefficient, "\n")
cat("P-value:", p_value, "\n")
cat("99% Confidence Interval:", conf_interval, "\n")
```

*Discuss the meaning of your analysis.*      

*Correlation Coefficient: 0.7086245*  

- There is a strong positive linear relationship between GrLivArea and SalePrice. This means that as the above-grade living area increases, the sale price tends to increase as well.  

*P-value: 4.518034e-223*  

- The p-value is extremely small (essentially zero), much smaller than the typical significance level (e.g., 0.01 for a 99% confidence interval).  
- This indicates that the correlation is highly statistically significant. We reject the null hypothesis that the correlation between GrLivArea and SalePrice is zero.  

*99% Confidence Interval: [0.6733974, 0.7406408]*  

- We are 99% confident that the true correlation coefficient lies between 0.6733974 and 0.7406408.  
- This range indicates a strong positive correlation, reaffirming the strength of the relationship between the two variables.  

*Conclusion*  
Based on the results from the correlation test and the confidence interval:  

- There is a strong, statistically significant positive correlation between the living area above grade (GrLivArea) and the sale price (SalePrice).  
- The 99% confidence interval for the correlation coefficient is [0.6733974, 0.7406408], indicating a strong and reliable positive relationship between these variables.  

## 4 Linear Algebra and Correlation. 

### 4.1 Correlation

**Invert your correlation matrix. (This is known as the precision matrix and contains variance inflation factors on the diagonal.)**    
```{r}
# Invert the correlation matrix to obtain the precision matrix
precision_matrix <- solve(correlation_matrix)

# Extract the diagonal elements (VIFs)
vifs <- diag(precision_matrix)


cat("Correlation Matrix:\n")
print(correlation_matrix)
cat("\nPrecision Matrix (Inverse of Correlation Matrix):\n")
print(precision_matrix)
cat("\nVariance Inflation Factors (VIFs):\n")
print(vifs)
```

*Interpretation*  

- The correlation matrix shows a strong positive correlation of 0.7086245 between GrLivArea and SalePrice. This indicates that as the above-grade living area increases, the sale price tends to increase as well.  

- The precision matrix (inverse of the correlation matrix) has diagonal elements greater than 1 (2.008632), which is typical when there's some collinearity. The off-diagonal elements are negative, reflecting the inverse relationship within the context of the precision matrix.   

- VIFs: Both GrLivArea and SalePrice have a VIF of approximately 2.008632.  

A VIF value close to 1 suggests low collinearity, while higher values indicate more severe collinearity. In this case, a VIF of 2.008632 indicates moderate collinearity. This means that there is some multicollinearity between GrLivArea and SalePrice, but it is not severe.   


Multiply the correlation matrix by the precision matrix, and then multiply the precision matrix by the correlation matrix.  
```{r}
# Multiplying the correlation matrix by the precision matrix (its inverse) and then the precision matrix by the correlation matrix should both yield the identity matrix, as the precision matrix is the inverse of the correlation matrix.

precision_matrix <- solve(correlation_matrix)

identity_matrix_1 <- correlation_matrix %*% precision_matrix
identity_matrix_2 <- precision_matrix %*% correlation_matrix

cat("Correlation Matrix:\n")
print(correlation_matrix)
cat("\nPrecision Matrix (Inverse of Correlation Matrix):\n")
print(precision_matrix)
cat("\nCorrelation Matrix * Precision Matrix:\n")
print(identity_matrix_1)
cat("\nPrecision Matrix * Correlation Matrix:\n")
print(identity_matrix_2)
```

The strong positive correlation (0.7086245) between GrLivArea and SalePrice indicates that larger living areas are associated with higher sale prices. The variance inflation factors (VIFs) around 2.008632 suggest moderate collinearity. Matrix multiplications confirm the precision matrix as the correct inverse of the correlation matrix.  

### 4.2 Principle Components Analysis (PSA)

**Conduct principle components analysis (research this!)  and interpret.  Discuss.**  
```{r}
# PCA
pca_result <- prcomp(variables, scale. = TRUE)

# Summary of PCA
summary(pca_result)

# PCA loadings
pca_result$rotation

pca_result$x

library(ggplot2)
biplot(pca_result)
```
*Interpretation:*

- The first principal component (PC1) captures 85.43% of the total variance in the data, which means it represents the most significant trend or pattern in the dataset.   

- The second principal component (PC2) captures the remaining 14.57% of the variance, indicating that it captures a secondary trend or pattern.  

- Both GrLivArea and SalePrice contribute equally to PC1, and the negative signs indicate that as PC1 increases, both GrLivArea and SalePrice decrease. This suggests that PC1 captures the overall scale of the properties, where larger values (in absolute terms) of PC1 correspond to properties with larger living areas and higher sale prices.  
- For PC2, GrLivArea and SalePrice have opposite signs. This means that PC2 captures the contrast between GrLivArea and SalePrice. Higher PC2 scores indicate properties with relatively larger living areas but lower sale prices, and vice versa.  

- The PCA biplot shows that GrLivArea and SalePrice both contribute equally to the primary trend (PC1), representing a combined measure of property size and value, while PC2 highlights the inverse relationship between them, indicating differences between properties with larger living areas but relatively lower prices and vice versa. The cluster of points around the center suggests that most properties have similar characteristics in terms of their living area and sale price, with a few properties showing variations as indicated by their spread along the principal components.    

## 5. Calculus-Based Probability & Statistics  

### 5.1 Fitted Exponential Distribution  

**Many times, it makes sense to fit a closed form distribution to data.**
- **For your variable that is skewed to the right, shift it so that the minimum value is above zero.**  

- **Then load the MASS package and run fitdistr to fit an exponential probability density function.**  
```{r}
skewness_grlivarea

# Shifting the data so that the minimum value is above zero
shifted_GrLivArea <- train_df$GrLivArea - min(train_df$GrLivArea) + 1

# fitdistr to fit an exponential distribution
fit <- fitdistr(shifted_GrLivArea, "exponential")

print(fit)
```

*Interpretation:*   
- Rate Parameter (λ): The fitted rate parameter (e.g., 8.456919e-04) of the exponential distribution.  

- Standard Error: The standard error of the estimated parameter (e.g., 2.213277e-05).  

This fitted exponential distribution can be used to model the distribution of the shifted GrLivArea data. The rate parameter indicates the expected rate of occurrence for the values in the data. The exponential distribution is characterized by a constant hazard rate, which in this context can help in understanding the distribution pattern of the GrLivArea variable after the shift.

### 5.2 Optimal Value of λ

**Find the optimal value of λ for this distribution, and then take 1000 samples from this exponential distribution using this value (e.g., rexp(1000, λ)).**  
```{r}
# The optimal value of lambda
lambda <- fit$estimate
cat("Optimal value of λ:", lambda, "\n")

# 1000 samples from the exponential distribution using the fitted λ
samples <- rexp(1000, rate = lambda)

print(head(samples))
```

### 5.3 Plotting and Comparison

**Plot a histogram and compare it with a histogram of your original variable.**  

```{r}
# Plot histograms
par(mfrow = c(1, 2))  # Set up a plotting area with 2 plots side by side

# Histogram of the original shifted data
hist(shifted_GrLivArea, breaks = 30, main = "Histogram of Shifted GrLivArea",
     xlab = "Shifted GrLivArea", col = "blue", border = "black", probability = TRUE)
lines(density(shifted_GrLivArea), col = "red", lwd = 2)  # Add a density line

# Histogram of the generated samples
hist(samples, breaks = 30, main = "Histogram of Generated Samples",
     xlab = "Generated Samples", col = "green", border = "black", probability = TRUE)
lines(density(samples), col = "red", lwd = 2)
```

*Comparison*  

- The histograms and density lines show a good visual match between the shifted original data and the generated samples, indicating that the exponential distribution is a reasonable fit for the right-skewed GrLivArea data.  

- The right skew and the overall shape of both distributions are similar, supporting the use of the exponential distribution to model the GrLivArea variable.  

### 5.4 PDF and CDF

**Using the exponential pdf, find the 5th and 95th percentiles using the cumulative distribution function (CDF).**  

```{r}

# The 5th and 95th percentiles
percentile_5th <- qexp(0.05, rate = lambda)
percentile_95th <- qexp(0.95, rate = lambda)

# Print the percentiles
cat("5th percentile:", percentile_5th, "\n")
cat("95th percentile:", percentile_95th, "\n")

```

**Also generate a 95% confidence interval from the empirical data, assuming normality.**  

```{r}

# The sample mean and standard deviation
sample_mean <- mean(shifted_GrLivArea)
sample_sd <- sd(shifted_GrLivArea)
n <- length(shifted_GrLivArea)  # Sample size

# t-critical value for a 95% confidence interval
alpha <- 0.05
t_critical <- qt(1 - alpha / 2, df = n - 1)

# Margin of error
margin_of_error <- t_critical * (sample_sd / sqrt(n))

# 95% confidence interval
ci_lower <- sample_mean - margin_of_error
ci_upper <- sample_mean + margin_of_error

cat("95% Confidence Interval: [", ci_lower, ", ", ci_upper, "]\n")

```

**Finally, provide the empirical 5th percentile and 95th percentile of the data.  Discuss.**  

```{r}
# The 5th and 95th percentiles
percentile_5th_empirical <- quantile(shifted_GrLivArea, 0.05)
percentile_95th_empirical <- quantile(shifted_GrLivArea, 0.95)

cat("Empirical 5th percentile:", percentile_5th_empirical, "\n")
cat("Empirical 95th percentile:", percentile_95th_empirical, "\n")

```

*Interpretation of Empirical Percentiles*  

- Empirical 5th Percentile (515): Indicates that 5% of the properties have a GrLivArea below 515, showing the lower bound for typical property sizes.  

- Empirical 95th Percentile (2133.1): Indicates that 95% of the properties have a GrLivArea below 2133.1, showing the upper bound for typical property sizes.  

*Comparison:*  
The empirical percentiles differ significantly from the theoretical percentiles (52.68 and 3523.99), suggesting the exponential model does not perfectly fit the data. The empirical data is more concentrated within a specific range.  

## 6. Modeling  

**Build some type of regression  model and submit your model to the competition board.**   
**Provide your complete model summary and results with analysis.**   
**Report your Kaggle.com  user name and score.**  

```{r}
# Log transform the skewed variables
train_df$SalePrice <- log(train_df$SalePrice)
train_df$GrLivArea <- log(train_df$GrLivArea)
test_df$GrLivArea <- log(test_df$GrLivArea)

# The relevant features for the model
train_data <- train_df[, c("SalePrice", "GrLivArea", "OverallQual", "YearBuilt", "TotalBsmtSF", "GarageCars", "FullBath", "TotRmsAbvGrd", 
                           "YearRemodAdd", "BsmtFinSF1", "X1stFlrSF", "X2ndFlrSF", "Fireplaces")]
test_data <- test_df[, c("Id", "GrLivArea", "OverallQual", "YearBuilt", "TotalBsmtSF", "GarageCars", "FullBath", "TotRmsAbvGrd", 
                         "YearRemodAdd", "BsmtFinSF1", "X1stFlrSF", "X2ndFlrSF", "Fireplaces")]

# Using median imputation to impute missing values 
preProcValues <- preProcess(train_data, method = c("medianImpute"))
train_data <- predict(preProcValues, train_data)
test_data <- predict(preProcValues, test_data)

# Our linear regression model
model <- lm(SalePrice ~ ., data = train_data)

summary(model)

# Predictions on the test dataset
predictions <- exp(predict(model, newdata = test_data))  # Inverse of log transformation

# Kaggle ubmission file
submission <- data.frame(Id = test_df$Id, SalePrice = predictions)
write.csv(submission, file = "haig_bedros_605_finals_submission.csv", row.names = FALSE)

# Print a message indicating completion
cat("Model built and submission file created.\n")
```